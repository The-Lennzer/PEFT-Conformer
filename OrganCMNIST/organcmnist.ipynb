{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\n\nfrom timm.models.layers import DropPath, trunc_normal_\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass ConvBlock(nn.Module):\n\n    def __init__(self, inplanes, outplanes, stride=1, res_conv=False, act_layer=nn.ReLU, groups=1,\n                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6), drop_block=None, drop_path=None):\n        super(ConvBlock, self).__init__()\n\n        expansion = 4\n        med_planes = outplanes // expansion\n\n        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn1 = norm_layer(med_planes)\n        self.act1 = act_layer(inplace=True)\n\n        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=stride, groups=groups, padding=1, bias=False)\n        self.bn2 = norm_layer(med_planes)\n        self.act2 = act_layer(inplace=True)\n\n        self.conv3 = nn.Conv2d(med_planes, outplanes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn3 = norm_layer(outplanes)\n        self.act3 = act_layer(inplace=True)\n\n        if res_conv:\n            self.residual_conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=stride, padding=0, bias=False)\n            self.residual_bn = norm_layer(outplanes)\n\n        self.res_conv = res_conv\n        self.drop_block = drop_block\n        self.drop_path = drop_path\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.bn3.weight)\n\n    def forward(self, x, x_t=None, return_x_2=True):\n        residual = x\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        x = self.act1(x)\n\n        x = self.conv2(x) if x_t is None else self.conv2(x + x_t)\n        x = self.bn2(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        x2 = self.act2(x)\n\n        x = self.conv3(x2)\n        x = self.bn3(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n\n        if self.res_conv:\n            residual = self.residual_conv(residual)\n            residual = self.residual_bn(residual)\n\n        x += residual\n        x = self.act3(x)\n\n        if return_x_2:\n            return x, x2\n        else:\n            return x\n\n\nclass FCUDown(nn.Module):\n    \"\"\" CNN feature maps -> Transformer patch embeddings\n    \"\"\"\n\n    def __init__(self, inplanes, outplanes, dw_stride, act_layer=nn.GELU,\n                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n        super(FCUDown, self).__init__()\n        self.dw_stride = dw_stride\n\n        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n        self.sample_pooling = nn.AvgPool2d(kernel_size=dw_stride, stride=dw_stride)\n\n        self.ln = norm_layer(outplanes)\n        self.act = act_layer()\n\n    def forward(self, x, x_t):\n        x = self.conv_project(x)  # [N, C, H, W]\n\n        x = self.sample_pooling(x).flatten(2).transpose(1, 2)\n        x = self.ln(x)\n        x = self.act(x)\n\n        x = torch.cat([x_t[:, 0][:, None, :], x], dim=1)\n\n        return x\n\n\nclass FCUUp(nn.Module):\n    \"\"\" Transformer patch embeddings -> CNN feature maps\n    \"\"\"\n\n    def __init__(self, inplanes, outplanes, up_stride, act_layer=nn.ReLU,\n                 norm_layer=partial(nn.BatchNorm2d, eps=1e-6),):\n        super(FCUUp, self).__init__()\n\n        self.up_stride = up_stride\n        self.conv_project = nn.Conv2d(inplanes, outplanes, kernel_size=1, stride=1, padding=0)\n        self.bn = norm_layer(outplanes)\n        self.act = act_layer()\n\n    def forward(self, x, H, W):\n        B, _, C = x.shape\n        # [N, 197, 384] -> [N, 196, 384] -> [N, 384, 196] -> [N, 384, 14, 14]\n        x_r = x[:, 1:].transpose(1, 2).reshape(B, C, H, W)\n        x_r = self.act(self.bn(self.conv_project(x_r)))\n\n        return F.interpolate(x_r, size=(H * self.up_stride, W * self.up_stride))\n\n\nclass Med_ConvBlock(nn.Module):\n    \"\"\" special case for Convblock with down sampling,\n    \"\"\"\n    def __init__(self, inplanes, act_layer=nn.ReLU, groups=1, norm_layer=partial(nn.BatchNorm2d, eps=1e-6),\n                 drop_block=None, drop_path=None):\n\n        super(Med_ConvBlock, self).__init__()\n\n        expansion = 4\n        med_planes = inplanes // expansion\n\n        self.conv1 = nn.Conv2d(inplanes, med_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn1 = norm_layer(med_planes)\n        self.act1 = act_layer(inplace=True)\n\n        self.conv2 = nn.Conv2d(med_planes, med_planes, kernel_size=3, stride=1, groups=groups, padding=1, bias=False)\n        self.bn2 = norm_layer(med_planes)\n        self.act2 = act_layer(inplace=True)\n\n        self.conv3 = nn.Conv2d(med_planes, inplanes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn3 = norm_layer(inplanes)\n        self.act3 = act_layer(inplace=True)\n\n        self.drop_block = drop_block\n        self.drop_path = drop_path\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.bn3.weight)\n\n    def forward(self, x):\n        residual = x\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        x = self.act1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        x = self.act2(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n\n        x += residual\n        x = self.act3(x)\n\n        return x\n\n\nclass ConvTransBlock(nn.Module):\n    \"\"\"\n    Basic module for ConvTransformer, keep feature maps for CNN block and patch embeddings for transformer encoder block\n    \"\"\"\n\n    def __init__(self, inplanes, outplanes, res_conv, stride, dw_stride, embed_dim, num_heads=12, mlp_ratio=4.,\n                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n                 last_fusion=False, num_med_block=0, groups=1):\n\n        super(ConvTransBlock, self).__init__()\n        expansion = 4\n        self.cnn_block = ConvBlock(inplanes=inplanes, outplanes=outplanes, res_conv=res_conv, stride=stride, groups=groups)\n\n        if last_fusion:\n            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, stride=2, res_conv=True, groups=groups)\n        else:\n            self.fusion_block = ConvBlock(inplanes=outplanes, outplanes=outplanes, groups=groups)\n\n        if num_med_block > 0:\n            self.med_block = []\n            for i in range(num_med_block):\n                self.med_block.append(Med_ConvBlock(inplanes=outplanes, groups=groups))\n            self.med_block = nn.ModuleList(self.med_block)\n\n        self.squeeze_block = FCUDown(inplanes=outplanes // expansion, outplanes=embed_dim, dw_stride=dw_stride)\n\n        self.expand_block = FCUUp(inplanes=embed_dim, outplanes=outplanes // expansion, up_stride=dw_stride)\n\n        self.trans_block = Block(\n            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=drop_path_rate)\n\n        self.dw_stride = dw_stride\n        self.embed_dim = embed_dim\n        self.num_med_block = num_med_block\n        self.last_fusion = last_fusion\n\n    def forward(self, x, x_t):\n        x, x2 = self.cnn_block(x)\n\n        _, _, H, W = x2.shape\n\n        x_st = self.squeeze_block(x2, x_t)\n\n        x_t = self.trans_block(x_st + x_t)\n\n        if self.num_med_block > 0:\n            for m in self.med_block:\n                x = m(x)\n\n        x_t_r = self.expand_block(x_t, H // self.dw_stride, W // self.dw_stride)\n        x = self.fusion_block(x, x_t_r, return_x_2=False)\n\n        return x, x_t\n\n\nclass Conformer(nn.Module):\n\n    def __init__(self, patch_size=16, in_chans=3, num_classes=1000, base_channel=64, channel_ratio=4, num_med_block=0,\n                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.):\n\n        # Transformer\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        assert depth % 3 == 0\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.trans_dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n\n        # Classifier head\n        self.trans_norm = nn.LayerNorm(embed_dim)\n        self.trans_cls_head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.conv_cls_head = nn.Linear(int(256 * channel_ratio), num_classes)\n\n        # Stem stage: get the feature maps by conv block (copied form resnet.py)\n        self.conv1 = nn.Conv2d(in_chans, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 1 / 2 [112, 112]\n        self.bn1 = nn.BatchNorm2d(64)\n        self.act1 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)  # 1 / 4 [56, 56]\n\n        # 1 stage\n        stage_1_channel = int(base_channel * channel_ratio)\n        trans_dw_stride = patch_size // 4\n        self.conv_1 = ConvBlock(inplanes=64, outplanes=stage_1_channel, res_conv=True, stride=1)\n        self.trans_patch_conv = nn.Conv2d(64, embed_dim, kernel_size=trans_dw_stride, stride=trans_dw_stride, padding=0)\n        self.trans_1 = Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n                             qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=self.trans_dpr[0],\n                             )\n\n        # 2~4 stage\n        init_stage = 2\n        fin_stage = depth // 3 + 1\n        for i in range(init_stage, fin_stage):\n            self.add_module('conv_trans_' + str(i),\n                    ConvTransBlock(\n                        stage_1_channel, stage_1_channel, False, 1, dw_stride=trans_dw_stride, embed_dim=embed_dim,\n                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n                        num_med_block=num_med_block\n                    )\n            )\n\n\n        stage_2_channel = int(base_channel * channel_ratio * 2)\n        # 5~8 stage\n        init_stage = fin_stage # 5\n        fin_stage = fin_stage + depth // 3 # 9\n        for i in range(init_stage, fin_stage):\n            s = 2 if i == init_stage else 1\n            in_channel = stage_1_channel if i == init_stage else stage_2_channel\n            res_conv = True if i == init_stage else False\n            self.add_module('conv_trans_' + str(i),\n                    ConvTransBlock(\n                        in_channel, stage_2_channel, res_conv, s, dw_stride=trans_dw_stride // 2, embed_dim=embed_dim,\n                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n                        num_med_block=num_med_block\n                    )\n            )\n\n        stage_3_channel = int(base_channel * channel_ratio * 2 * 2)\n        # 9~12 stage\n        init_stage = fin_stage  # 9\n        fin_stage = fin_stage + depth // 3  # 13\n        for i in range(init_stage, fin_stage):\n            s = 2 if i == init_stage else 1\n            in_channel = stage_2_channel if i == init_stage else stage_3_channel\n            res_conv = True if i == init_stage else False\n            last_fusion = True if i == depth else False\n            self.add_module('conv_trans_' + str(i),\n                    ConvTransBlock(\n                        in_channel, stage_3_channel, res_conv, s, dw_stride=trans_dw_stride // 4, embed_dim=embed_dim,\n                        num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                        drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, drop_path_rate=self.trans_dpr[i-1],\n                        num_med_block=num_med_block, last_fusion=last_fusion\n                    )\n            )\n        self.fin_stage = fin_stage\n\n        trunc_normal_(self.cls_token, std=.02)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n        elif isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1.)\n            nn.init.constant_(m.bias, 0.)\n        elif isinstance(m, nn.GroupNorm):\n            nn.init.constant_(m.weight, 1.)\n            nn.init.constant_(m.bias, 0.)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'cls_token'}\n\n\n    def forward(self, x):\n        B = x.shape[0]\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n\n        # pdb.set_trace()\n        # stem stage [N, 3, 224, 224] -> [N, 64, 56, 56]\n        x_base = self.maxpool(self.act1(self.bn1(self.conv1(x))))\n\n        # 1 stage\n        x = self.conv_1(x_base, return_x_2=False)\n\n        x_t = self.trans_patch_conv(x_base).flatten(2).transpose(1, 2)\n        x_t = torch.cat([cls_tokens, x_t], dim=1)\n        x_t = self.trans_1(x_t)\n\n        # 2 ~ final\n        for i in range(2, self.fin_stage):\n            x, x_t = eval('self.conv_trans_' + str(i))(x, x_t)\n\n        # conv classification\n        x_p = self.pooling(x).flatten(1)\n        conv_cls = self.conv_cls_head(x_p)\n\n        # trans classification\n        x_t = self.trans_norm(x_t)\n        tran_cls = self.trans_cls_head(x_t[:, 0])\n\n        return [conv_cls, tran_cls]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:44:55.604344Z","iopub.execute_input":"2025-01-01T17:44:55.604686Z","iopub.status.idle":"2025-01-01T17:44:55.648055Z","shell.execute_reply.started":"2025-01-01T17:44:55.604657Z","shell.execute_reply":"2025-01-01T17:44:55.647189Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install medmnist","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:44:55.649154Z","iopub.execute_input":"2025-01-01T17:44:55.649385Z","iopub.status.idle":"2025-01-01T17:45:01.850890Z","shell.execute_reply.started":"2025-01-01T17:44:55.649365Z","shell.execute_reply":"2025-01-01T17:45:01.850074Z"}},"outputs":[{"name":"stdout","text":"Collecting medmnist\n  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.1.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.2.2)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.24.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.5)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (10.4.0)\nCollecting fire (from medmnist)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.4.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.1+cu121)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.1)\nRequirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.13.1)\nRequirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.3)\nRequirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.35.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.8.30)\nRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (24.1)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (0.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.13.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2024.6.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->medmnist) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\nDownloading medmnist-3.0.2-py3-none-any.whl (25 kB)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114250 sha256=6ff7df3a856bea2e813710f56dd239b2ac8ac0d6ed8bd0caa9e50628150dc5e9\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built fire\nInstalling collected packages: fire, medmnist\nSuccessfully installed fire-0.7.0 medmnist-3.0.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from medmnist import OrganCMNIST\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch\n\n# Define transforms for grayscale images\ntrain_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(10),\n    # Convert grayscale to 3 channels\n    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if isinstance(x, torch.Tensor) else transforms.ToTensor()(x).repeat(3, 1, 1)),\n    transforms.RandomAffine(\n        degrees=0,\n        translate=(0.1, 0.1),\n        scale=(0.9, 1.1),\n        shear=10\n    ),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Transform for validation and test (without augmentation)\nother_transforms = transforms.Compose([\n    # Convert grayscale to 3 channels\n    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if isinstance(x, torch.Tensor) else transforms.ToTensor()(x).repeat(3, 1, 1)),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Load datasets\ntrain_dataset = OrganCMNIST(split=\"train\", download=True, transform=train_transforms, size=224)\nval_dataset = OrganCMNIST(split=\"val\", transform=other_transforms, download=True, size=224)\ntest_dataset = OrganCMNIST(split=\"test\", transform=other_transforms, download=True, size=224)\n\n# Create data loaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n# Test the data loading\nimage, label = train_dataset[0]\nprint(f\"Image shape: {image.shape}, Label: {label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:45:01.852705Z","iopub.execute_input":"2025-01-01T17:45:01.852999Z","iopub.status.idle":"2025-01-01T17:46:00.523431Z","shell.execute_reply.started":"2025-01-01T17:45:01.852976Z","shell.execute_reply":"2025-01-01T17:46:00.522686Z"}},"outputs":[{"name":"stdout","text":"Downloading https://zenodo.org/records/10519652/files/organcmnist_224.npz?download=1 to /root/.medmnist/organcmnist_224.npz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 760231860/760231860 [00:45<00:00, 16765995.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Using downloaded and verified file: /root/.medmnist/organcmnist_224.npz\nUsing downloaded and verified file: /root/.medmnist/organcmnist_224.npz\nImage shape: torch.Size([3, 224, 224]), Label: [5]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initialize model\nmodel = Conformer(\n    patch_size=16,\n    in_chans=3,\n    num_classes=11,\n    embed_dim=384,\n    depth=12,\n    num_heads=6,\n    mlp_ratio=4,\n    qkv_bias=True,\n    num_med_block=0\n)\nmodel = model.to(device)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Number of parameters: {total_params}\")\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.05)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='max',          # Since we're monitoring validation accuracy\n    factor=0.5,         # Multiply LR by this factor when plateauing\n    patience=2,         # Number of epochs to wait before reducing LR\n    verbose=True,       # Print message when LR is reduced\n    min_lr=1e-6        # Lower bound on the learning rate\n)\n\ndef train_one_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    pbar = tqdm(train_loader, desc='Training')\n\n    for batch_idx, (inputs, targets) in enumerate(pbar):\n        if inputs.size(0) == 0 or targets.size(0) == 0:\n            continue\n\n        inputs = inputs.to(device)\n        targets = targets.squeeze().to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n\n        # Convert list outputs to tensors if necessary\n        if isinstance(outputs, list):\n            outputs = [out.to(device) if isinstance(out, torch.Tensor) else torch.tensor(out).to(device) \n                      for out in outputs]\n            loss = sum(criterion(out, targets) for out in outputs)\n        else:\n            loss = criterion(outputs, targets)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        pbar.set_postfix({'loss': loss.item()})\n\n    return running_loss / len(train_loader)\n\n         \ndef evaluate(model, data_loader, device):\n    model.eval()\n    all_preds = []\n    all_targets = []\n    all_probs = []\n\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            #inputs = inputs.repeat(1, 3, 1, 1)\n            inputs, targets = inputs.to(device), targets.squeeze().to(device)\n\n            outputs = model(inputs)\n            # Average the predictions from both heads\n            probs = (torch.softmax(outputs[0], dim=1) + torch.softmax(outputs[1], dim=1)) / 2\n            preds = torch.argmax(probs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(targets.cpu().numpy())\n            all_probs.extend(probs.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_targets), np.array(all_probs)\n\ndef plot_metrics(all_preds, all_targets, all_probs, save_prefix='test'):\n    # Calculate metrics\n    accuracy = accuracy_score(all_targets, all_preds)\n    precision = precision_score(all_targets, all_preds, average='weighted')\n    recall = recall_score(all_targets, all_preds, average='weighted')\n    f1 = f1_score(all_targets, all_preds, average='weighted')\n\n    # Calculate ROC curve and AUC (for multi-class)\n    n_classes = all_probs.shape[1]\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve((all_targets == i).astype(int), all_probs[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Plot ROC curve\n    plt.figure(figsize=(10, 8))\n    for i in range(n_classes):\n        plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend()\n    plt.savefig(f'{save_prefix}_roc_curve.png')\n    plt.close()\n\n    # Plot confusion matrix\n    cm = confusion_matrix(all_targets, all_preds)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n    plt.savefig(f'{save_prefix}_confusion_matrix.png')\n    plt.close()\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'auc': roc_auc\n    }\n\n# Training loop\nnum_epochs = 100\nbest_val_acc = 0\npatience = 10\npatience_counter = 0\n\nfor epoch in range(num_epochs):\n    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n\n    # Train\n    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n\n    # Validate\n    val_preds, val_targets, val_probs = evaluate(model, val_loader, device)\n    val_metrics = plot_metrics(val_preds, val_targets, val_probs, save_prefix=f'val_epoch_{epoch}')\n\n    print(f'Train Loss: {train_loss:.4f}')\n    print(f'Validation Metrics:')\n    print(f'Accuracy: {val_metrics[\"accuracy\"]:.4f}')\n    print(f'Precision: {val_metrics[\"precision\"]:.4f}')\n    print(f'Recall: {val_metrics[\"recall\"]:.4f}')\n    print(f'F1 Score: {val_metrics[\"f1\"]:.4f}')\n\n    scheduler.step(val_metrics['accuracy'])\n\n    # Early stopping\n    if val_metrics['accuracy'] > best_val_acc:\n        best_val_acc = val_metrics['accuracy']\n        torch.save(model.state_dict(), 'best_model.pth')\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print('Early stopping triggered')\n            break\n\n# Load best model and evaluate on test set\nmodel.load_state_dict(torch.load('best_model.pth'))\ntest_preds, test_targets, test_probs = evaluate(model, test_loader, device)\ntest_metrics = plot_metrics(test_preds, test_targets, test_probs, save_prefix='test_final')\n\nprint('\\nTest Metrics:')\nprint(f'Accuracy: {test_metrics[\"accuracy\"]:.4f}')\nprint(f'Precision: {test_metrics[\"precision\"]:.4f}')\nprint(f'Recall: {test_metrics[\"recall\"]:.4f}')\nprint(f'F1 Score: {test_metrics[\"f1\"]:.4f}')\nprint('AUC scores:', {f'Class {k}': v for k, v in test_metrics['auc'].items()})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T17:46:00.524278Z","iopub.execute_input":"2025-01-01T17:46:00.524880Z","iopub.status.idle":"2025-01-01T21:43:08.148930Z","shell.execute_reply.started":"2025-01-01T17:46:00.524854Z","shell.execute_reply":"2025-01-01T21:43:08.147902Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Number of parameters: 36278934\n\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:43<00:00,  2.01it/s, loss=2.67] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.6271\nValidation Metrics:\nAccuracy: 0.7274\nPrecision: 0.7992\nRecall: 0.7274\nF1 Score: 0.6837\n\nEpoch 2/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.00it/s, loss=0.435]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5966\nValidation Metrics:\nAccuracy: 0.8127\nPrecision: 0.8348\nRecall: 0.8127\nF1 Score: 0.8057\n\nEpoch 3/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.01it/s, loss=0.878]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.2443\nValidation Metrics:\nAccuracy: 0.9151\nPrecision: 0.9275\nRecall: 0.9151\nF1 Score: 0.9126\n\nEpoch 4/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.01it/s, loss=0.546]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.1004\nValidation Metrics:\nAccuracy: 0.9448\nPrecision: 0.9484\nRecall: 0.9448\nF1 Score: 0.9447\n\nEpoch 5/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.00it/s, loss=1.58] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9973\nValidation Metrics:\nAccuracy: 0.9423\nPrecision: 0.9480\nRecall: 0.9423\nF1 Score: 0.9418\n\nEpoch 6/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=1.56] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9506\nValidation Metrics:\nAccuracy: 0.9385\nPrecision: 0.9424\nRecall: 0.9385\nF1 Score: 0.9392\n\nEpoch 7/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=2.14] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8896\nValidation Metrics:\nAccuracy: 0.9156\nPrecision: 0.9301\nRecall: 0.9156\nF1 Score: 0.9122\n\nEpoch 8/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.00it/s, loss=0.933]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6661\nValidation Metrics:\nAccuracy: 0.9615\nPrecision: 0.9657\nRecall: 0.9615\nF1 Score: 0.9612\n\nEpoch 9/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.494] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6336\nValidation Metrics:\nAccuracy: 0.9661\nPrecision: 0.9686\nRecall: 0.9661\nF1 Score: 0.9659\n\nEpoch 10/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.258]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6114\nValidation Metrics:\nAccuracy: 0.9670\nPrecision: 0.9678\nRecall: 0.9670\nF1 Score: 0.9668\n\nEpoch 11/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.00it/s, loss=0.461]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5939\nValidation Metrics:\nAccuracy: 0.9695\nPrecision: 0.9713\nRecall: 0.9695\nF1 Score: 0.9695\n\nEpoch 12/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:46<00:00,  2.00it/s, loss=0.297] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5624\nValidation Metrics:\nAccuracy: 0.9804\nPrecision: 0.9814\nRecall: 0.9804\nF1 Score: 0.9804\n\nEpoch 13/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.56]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.5377\nValidation Metrics:\nAccuracy: 0.9766\nPrecision: 0.9783\nRecall: 0.9766\nF1 Score: 0.9768\n\nEpoch 14/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.00it/s, loss=0.74]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4988\nValidation Metrics:\nAccuracy: 0.9841\nPrecision: 0.9855\nRecall: 0.9841\nF1 Score: 0.9843\n\nEpoch 15/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.468] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4896\nValidation Metrics:\nAccuracy: 0.9737\nPrecision: 0.9752\nRecall: 0.9737\nF1 Score: 0.9738\n\nEpoch 16/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.703] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4736\nValidation Metrics:\nAccuracy: 0.9699\nPrecision: 0.9743\nRecall: 0.9699\nF1 Score: 0.9704\n\nEpoch 17/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.00it/s, loss=0.674] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4473\nValidation Metrics:\nAccuracy: 0.9749\nPrecision: 0.9766\nRecall: 0.9749\nF1 Score: 0.9749\n\nEpoch 18/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.00it/s, loss=0.359] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3382\nValidation Metrics:\nAccuracy: 0.9829\nPrecision: 0.9840\nRecall: 0.9829\nF1 Score: 0.9830\n\nEpoch 19/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.01it/s, loss=0.539]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.3112\nValidation Metrics:\nAccuracy: 0.9745\nPrecision: 0.9768\nRecall: 0.9745\nF1 Score: 0.9747\n\nEpoch 20/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:46<00:00,  2.00it/s, loss=0.228] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2933\nValidation Metrics:\nAccuracy: 0.9833\nPrecision: 0.9838\nRecall: 0.9833\nF1 Score: 0.9832\n\nEpoch 21/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.195]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2499\nValidation Metrics:\nAccuracy: 0.9770\nPrecision: 0.9785\nRecall: 0.9770\nF1 Score: 0.9771\n\nEpoch 22/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:44<00:00,  2.00it/s, loss=0.0488] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2317\nValidation Metrics:\nAccuracy: 0.9808\nPrecision: 0.9818\nRecall: 0.9808\nF1 Score: 0.9808\n\nEpoch 23/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.182]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2184\nValidation Metrics:\nAccuracy: 0.9866\nPrecision: 0.9877\nRecall: 0.9866\nF1 Score: 0.9868\n\nEpoch 24/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:46<00:00,  1.99it/s, loss=0.232]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2116\nValidation Metrics:\nAccuracy: 0.9845\nPrecision: 0.9858\nRecall: 0.9845\nF1 Score: 0.9847\n\nEpoch 25/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:46<00:00,  2.00it/s, loss=0.0374] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.2040\nValidation Metrics:\nAccuracy: 0.9829\nPrecision: 0.9835\nRecall: 0.9829\nF1 Score: 0.9828\n\nEpoch 26/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:46<00:00,  2.00it/s, loss=0.00774]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1954\nValidation Metrics:\nAccuracy: 0.9858\nPrecision: 0.9864\nRecall: 0.9858\nF1 Score: 0.9858\n\nEpoch 27/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.635]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1679\nValidation Metrics:\nAccuracy: 0.9837\nPrecision: 0.9844\nRecall: 0.9837\nF1 Score: 0.9837\n\nEpoch 28/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.213]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1592\nValidation Metrics:\nAccuracy: 0.9833\nPrecision: 0.9843\nRecall: 0.9833\nF1 Score: 0.9833\n\nEpoch 29/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.0339] \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1523\nValidation Metrics:\nAccuracy: 0.9820\nPrecision: 0.9830\nRecall: 0.9820\nF1 Score: 0.9821\n\nEpoch 30/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:47<00:00,  1.99it/s, loss=0.157]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1359\nValidation Metrics:\nAccuracy: 0.9845\nPrecision: 0.9857\nRecall: 0.9845\nF1 Score: 0.9847\n\nEpoch 31/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:46<00:00,  2.00it/s, loss=0.0191]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1327\nValidation Metrics:\nAccuracy: 0.9808\nPrecision: 0.9818\nRecall: 0.9808\nF1 Score: 0.9808\n\nEpoch 32/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:45<00:00,  2.00it/s, loss=0.32]   \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1272\nValidation Metrics:\nAccuracy: 0.9816\nPrecision: 0.9825\nRecall: 0.9816\nF1 Score: 0.9816\n\nEpoch 33/100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 811/811 [06:46<00:00,  1.99it/s, loss=0.0373]  \n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1220\nValidation Metrics:\nAccuracy: 0.9820\nPrecision: 0.9834\nRecall: 0.9820\nF1 Score: 0.9821\nEarly stopping triggered\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-5-c53920a71a0b>:181: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"\nTest Metrics:\nAccuracy: 0.9175\nPrecision: 0.9195\nRecall: 0.9175\nF1 Score: 0.9166\nAUC scores: {'Class 0': 0.9804126485304542, 'Class 1': 0.9957151819415945, 'Class 2': 0.9976670592483458, 'Class 3': 0.9943431671742803, 'Class 4': 0.9835568095012529, 'Class 5': 0.9887380911460962, 'Class 6': 0.9997734247662189, 'Class 7': 0.9990456580291234, 'Class 8': 0.9997541058348176, 'Class 9': 0.9948799892847575, 'Class 10': 0.9956774869926235}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}